---
layout: post
title: 机器学习实战1 - 决策树
category: science
tag: ml
---

## 决策树(Decision Tree)

[chapter03][chapter03]
[chapter09][chapter09]

决策树算法也是一种比较直观的机器学习算法,可以用于分类(classification)或回归(regression).决策树的一个最大的优点是可以显式的总结出数据体现的规则,这样的规则有助于人们更进一步的理解数据.另一个优点是快速,相比于k最近邻每次都需要全部数据,决策树只需要树高最大为属性数目的树形结构即可,小而且快速.

决策树很类似于我们平时做判断的过程,当我们面对众多属性时,我们肯定会逐个属性的进行分析,要不就是基于非常明显的特征,直接得到结论,要不就是逐步的排除不可能的结论,直到最后的结论.具体步骤如下:

1. 挑选一个最优的可以被用来划分的属性
2. 以该属性值将既有数据划分,并构建当前节点的子树,每个子树对应划分好的子数据集
3. 递归的处理每个子树和子数据集

这只是决策树的**训练步骤**.与k最近邻不同,决策树是需要事先通过训练集训练的(当然,k最近邻其实也要训练,比如前文说的如何寻找k值,就是训练的过程).之后,才能用来进行测试集的测试.**使用**的具体步骤如下:

1. 取当前节点属性对应的数据的值
2. 根据值,选择对应的子树
3. 递归处理子树,直到到叶子节点,得到最后结论

可以看到,使用的时候,最多经过属性数目次的递归操作,就能得到最后的结果,而且整个数据集的对应的特征,已经被树形结构抽象概括出来了,也就无需数据集参与了.其实这本质上很像基于规则的机器学习,只不过此处的规则,是基于属性值的划分而已.

决策树同样有3个需要注意的问题

### 如何选择划分的属性

事实上,决策树的众多算法的区别的重点,就在于此.书上[第3章][chapter03]介绍的ID3算法(Iterative Dichotomiser 3)和[第9章][chapter09]介绍的CART算法(Classification And Regression Trees),都是从不同的角度看待基于属性划分后的优劣情况,或者用[机器学习][mitchell]上的说法,使用的不同的归纳偏置(inductive bias)

* ID3算法: 划分后信息增益(information gain)越高,越好.
* CART算法: 划分后错误率越低,越好.

这两种算法,从结果来看,都是试图用贪心的方法,构建一个比较矮的树,所以二者的归纳偏置可以总结为:越矮的树,越好.就和梯度下降(gradient descent)一样,难免陷入局部最优,这个我们另说.

#### 信息增益

ID3使用的信息增益,是一个信息论的常用量,是指信息熵(entropy)的减少.信息熵一般可以理解为信息的混乱程度,具体公式为:

$$
E(D) = - \sum _ {i = 1} ^ {n} P(D _ i) * log _ 2 (P(D _ i))
$$







[chapter03]: https://github.com/LelouchHe/machine_learning_in_action_code/tree/master/chapter03
[chapter09]: https://github.com/LelouchHe/machine_learning_in_action_code/tree/master/chapter09
