---
layout: math
title: 机器学习实战3 - 朴素贝叶斯
category: science
tag: ml
---

## 朴素贝叶斯(Naive Bayes)

[chapter04][chapter04]

朴素贝叶斯算法,和前面的k最近邻和决策树相比,数学味道重了很多.其数学基础,[贝叶斯法则][bayes],通过巧妙结合以往经验与现实数据,来评估可能的假设分布.

## 贝叶斯法则

贝叶斯法则是用来计算[条件概率(conditional probability)][cond]的一套方法,条件概率$P(A\|B)$是指在B发生的前提之下,A发生的概率情况,其公式为:

$$
P(A | B) = \frac{P(B | A)P(A)}{P(B)}
$$

贝叶斯其实是利用另一个条件概率$P(B|A)$来计算目标条件概率.乍看之下,貌似解决不了问题,但此处的关键在于,条件概率计算的难度不一样,通过贝叶斯法则,我们就可以利用一个较为容易获得的条件概率计算出较难获得的条件概率.这其实很符合科学研究的方法.当某一个问题比较难以处理的时候,通过证明其等价的容易的问题来解决,这个思路是一致的

回到机器学习的角度上来说,在既有数据集(D)和所有可能假设(H)的情况下,我们试图得到一个最为可能的假设(h),这可以看作是在求:

$$
h = \argmax _ {h \in H} P(h | D)
$$

抽象的来说,任何学习其实都是在求解这个h.但这个值的求解是比较困难的(单机器学习就汗牛充栋了,不要提其他方法了),所以利用贝叶斯法则,我们做一下转换:

$$

h = \argmax _ {h \in H} P(h | D)
  = \argmax _ {h \in H} \frac{P(D | h)P(h)}{P(D)}
  = \argmax _ {h \in H} P(D | h)P(h)

$$

$P(D)$对于既有数据集来说,是常量,可以忽略.

可以看到,本质而言,我们是通过$P(h)$(h本身成立的概率)和$P(D | h)$(h与D吻合的概率)来计算$P(h | D)$.用术语来说:

* $P(h)$: 假设h成立的**先验概率**,反映的是我们对h成立的背景知识,这个在我们试图获得真正的h之前,就已经有了.比如我们可以通过观察数据,或者干脆假设所有的假设都有同样的可能(此时$P(h) = 1 / | H |$)
* $P(D | h)$: 假设h成立的前提下,观察到D的概率,反映的是假设与数据吻合的情况.我们可以假设h是成立的,然后验证既有数据集,判断其成立的概率.
* $P(h | D)$: 在既有数据集下,假设h成立的概率.反映的是假设h在D上成立的可能.不同于先验概率,这个是经过数据验证的,称之为**后验概率**.能让后验概率最大化的假设,我们称之为极大后验(Maximum A Posteriori)假设,即$h _ {MAP}$

这样,我们通过2个比较容易获得的量$P(h)$与$P(D | h)$,来求比较困难的$h _ {MAP}$:

$$
h _ {MAP} = \argmax _ {h \in H} P(D | h)P(h)
$$

当我们对h没有任何背景知识时,可以继续简化为:

$$
h _ {ML} = \argmax _ {h \in H} P(D | h)
$$

















[chapter04]: https://github.com/LelouchHe/machine_learning_in_action_code/tree/master/chapter04
[bayes]: https://en.wikipedia.org/wiki/Bayes%27_theorem
[cond]: https://en.wikipedia.org/wiki/Conditional_probability